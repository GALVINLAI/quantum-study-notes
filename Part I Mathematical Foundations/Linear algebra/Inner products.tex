%åŽŸ2.1.4

% An inner product is a function which takes as input two vectors $|v\rangle$ and $|w\rangle$ from a vector space and produces a complex number as output. 

% We will see shortly that the matrix representation of dual vectors is just a row vector.

A function $(\cdot, \cdot)$ from $V \times V$ to $\mathbf{C}$ is an inner product if it satisfies the requirements that:
\begin{enumerate}
    \item $(\cdot, \cdot)$ is linear in the \textit{second} argument, 
\begin{equation}
    \left(|v\rangle, \sum_{i} \lambda_{i}\left|w_{i}\right\rangle\right)=\sum_{i} \lambda_{i}\left(|v\rangle,\left|w_{i}\right\rangle\right)
\end{equation}
    \item $(|v\rangle,|w\rangle)=(|w\rangle,|v\rangle)^{*}$.
    \item $(|v\rangle,|v\rangle) \geq 0$ with equality if and only if $|v\rangle=0$.
\end{enumerate}
We call a vector space equipped with an inner product an inner product space.

\begin{example}
    For example, $\mathbf{C}^{n}$ has an inner product defined by
\begin{equation}
    \left(\left(y_{1}, \ldots, y_{n}\right),\left(z_{1}, \ldots, z_{n}\right)\right) \equiv \sum_{i} y_{i}^{*} z_{i}=\left[y_{1}^{*} \ldots y_{n}^{*}\right]\left[\begin{array}{c}
z_{1} \\
\vdots \\
z_{n}
\end{array}\right].
\end{equation}
\end{example}

The standard quantum mechanical notation for the inner product $(|v\rangle,|w\rangle)$ is \begin{equation}
    \langle v | w\rangle,
\end{equation}where $|v\rangle$ and $|w\rangle$ are vectors in the inner product space, and the notation 
\begin{equation}
    \langle v|
\end{equation}
is used for the \textit{dual vector} to the vector $|v\rangle$; the dual is a linear operator from the inner product space $V$ to the complex numbers $\mathbf{C}$, defined by
\begin{equation}
    \langle v|(|w\rangle) \equiv\langle v | w\rangle \equiv(|v\rangle,|w\rangle).
\end{equation}

% Exercise 2.5: Verify that $(\cdot, \cdot)$ just defined is an inner product on $\mathbf{C}^{n}$.

\begin{exercise}
    Exercise 2.6: Show that any inner product $(\cdot, \cdot)$ is conjugate-linear in the \textit{first} argument\footnote{In some textbook of linear algebra, by definition they let inner product satisfy the linearity in the first argument. Then, like Exercise 2.6, we can show it is conjugate-linear in the second argument. Those two types of ways of definitions are indeed equivalent. },
\begin{equation}
    \left(\sum_{i} \lambda_{i}\left|w_{i}\right\rangle,|v\rangle\right)=\sum_{i} \lambda_{i}^{*}\left(\left|w_{i}\right\rangle,|v\rangle\right) .
\end{equation}
\end{exercise}

Discussions of quantum mechanics often refer to Hilbert space. In the finite dimensional complex vector spaces, a Hilbert space is exactly the same thing as an inner product space. From now on we use the two terms interchangeably, preferring the term Hilbert space.

% For example, $|w\rangle \equiv$ $(1,0)$ and $|v\rangle \equiv(0,1)$ are orthogonal with respect to the inner product defined by (2.14). 

We define the norm of a vector $|v\rangle$ by
\begin{equation}
    \||v\rangle \| \equiv \sqrt{\langle v | v\rangle}.
\end{equation}
A unit vector is a vector $|v\rangle$ such that $\||v\rangle \|=1$. We also say that $|v\rangle$ is normalized if $\| v\rangle \|=1$. It is convenient to talk of normalizing a vector by dividing by its norm; thus $|v\rangle / \||v\rangle \|$ is the normalized form of $|v\rangle$, for any non-zero vector $|v\rangle$. 
Vectors $|w\rangle$ and $|v\rangle$ are orthogonal if their inner product is zero. A set $|i\rangle$ of vectors with index $i$ is orthonormal if each vector is a unit vector, and distinct vectors in the set are orthogonal, that is, $\langle i | j\rangle=\delta_{i j}$, where $i$, $j$ are both chosen from the index set.

% Exercise 2.7: Verify that $|w\rangle \equiv(1,1)$ and $|v\rangle \equiv(1,-1)$ are orthogonal. What are the normalized forms of these vectors?

\begin{proposition}[Gram-Schmidt procedure]
    Suppose $\left|w_{1}\right\rangle, \ldots,\left|w_{d}\right\rangle$ is a basis set for some vector space $V$ with an inner product. There is a useful method, the Gram-Schmidt procedure, which can be used to produce an orthonormal basis set $\left|v_{1}\right\rangle, \ldots,\left|v_{d}\right\rangle$ for the vector space $V$. Define $\left|v_{1}\right\rangle \equiv\left|w_{1}\right\rangle / \|\left|w_{1}\right\rangle \|$, and for $1 \leq k \leq d-1$ define $\left|v_{k+1}\right\rangle$ inductively by
\begin{equation}
    \left|v_{k+1}\right\rangle \equiv \frac{\left|w_{k+1}\right\rangle-\sum_{i=1}^{k}\left\langle v_{i} | w_{k+1}\right\rangle\left|v_{i}\right\rangle}{\|\left|w_{k+1}\right\rangle-\sum_{i=1}^{k}\left\langle v_{i} | w_{k+1}\right\rangle\left|v_{i}\right\rangle \|}.
\end{equation}
\end{proposition}

It is not difficult to verify that the vectors $\left|v_{1}\right\rangle, \ldots,\left|v_{d}\right\rangle$ above form an orthonormal set which is also a basis for $V$. Thus, any finite dimensional vector space of dimension $d$ has an orthonormal basis, $\left|v_{1}\right\rangle, \ldots,\left|v_{d}\right\rangle$.

% Exercise 2.8: Prove that the Gram-Schmidt procedure produces an orthonormal basis for $V$.

\begin{remark}
    From now on, when we speak of a matrix representation for a linear operator, we mean a matrix representation with respect to orthonormal input and output bases. We also use the convention that if the input and output spaces for a linear operator are the same, then the input and output bases are the same, unless noted otherwise.
\end{remark}

With these conventions, the inner product on a Hilbert space can be given a convenient matrix representation. 

Let $|w\rangle=\sum_{i} w_{i}|i\rangle$ and $|v\rangle=\sum_{j} v_{j}|j\rangle$ be representations of vectors $|w\rangle$ and $|v\rangle$ with respect to some same orthonormal basis. Then, since $\langle i | j\rangle=\delta_{i j}$, we have
\begin{equation}
    \langle v | w\rangle 
=\left(\sum_{i} v_{i}|i\rangle, \sum_{j} w_{j}|j\rangle\right)=\sum_{i j} v_{i}^{*} w_{j} \delta_{i j}
=\sum_{i} v_{i}^{*} w_{i}
=\left[v_{1}^{*} \ldots v_{n}^{*}\right]\left[\begin{array}{c}
w_{1} \\
\vdots \\
w_{n}
\end{array}\right].
\end{equation}
That is, the inner product of two vectors is equal to the vector inner product between two matrix representations of those vectors, provided the representations are written with respect to the same orthonormal basis. 

% We also see that the dual vector $\langle v|$ has a nice interpretation as the row vector whose components are complex conjugates of the corresponding components of the column vector representation of $|v\rangle$.