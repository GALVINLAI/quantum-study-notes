
There are many important functions which can be defined for operators and matrices. 

\subsubsection{Operator functions}

Let $A=\sum_{a} a|a\rangle\langle a|$ be a spectral decomposition for a normal operator $A$. Is this decomposition form unique for the normal operator $A$? Actually, if there is an eigenspace is more than one dimensional, that is, there are more than one orthonormal eigenvectors sharing the same eigenvalue (in this case, we say $A$ is \textit{degenerate}); then, the spectral decomposition of $A$ is not unique (For more detail, see \href{https://physics.stackexchange.com/questions/576287/uniqueness-of-spectral-decomposition}{here}). But the eigenspaces corresponding to each eigenvalue are fixed. We can consider the unique spectral decomposition by using the notations of projectors of eigenspaces as following. Let 
$$A=\sum_{a} a P_{a}$$
be a spectral decomposition for a normal operator $A$, where all numbers $a$'s over the sum are distinct, and operator $P_{a}$ denotes the projector of eigenspace $V_a$ of $a.$ If $\dim V_a =d$, then we can construct projector $P_a:= \sum_{i=1}^{d} |a_{i}\rangle\langle a_{i}|$. We can show that the projector $P_a$ is invariant under the choosing arbitrary orthonormal basis $|a_{i}\rangle$ of eigenspace $V_a$. (To see the invariability, You can refer to the Lemma 2.1.3 in Zhijian Lai's master thesis at \href{https://galvinlai.github.io/files/doc/master_thesis_2021.pdf}{here}.)

Now we are ready to define the operator functions, or say, matrix function as below.

Given a function $f$ from the complex numbers to the complex numbers, it is possible to define a corresponding \textit{matrix function} on the set of normal matrices (or some subclass, such as the vector space of Hermitian matrices) by the following construction. 

Let $A=\sum_{a} a|a\rangle\langle a|$ be a spectral decomposition for a normal operator $A$. Define 
$$
f(A) \equiv \sum_{a} f(a)|a\rangle\langle a|.
$$
A little thought shows that $f(A)$ is uniquely defined. Because, we can rewrite the decomposition as$A=\sum_{a} a P_{a}$, then 
$$
f(A) \equiv \sum_{a} f(a) P_{a}
$$
where the numbers $f(a)$ and projectors $P_{a}$ are uniquely determined by $A$ itself, instead of the choose of orthogonal basis $|a\rangle$ of $V.$ This procedure can be used, for example, to define 
\begin{itemize}
    \item the square root of a positive operator $A$ (whose all eigenvalues are non-negative):
$$
\sqrt{A} \equiv \sum_{a} \sqrt{a} |a\rangle\langle a|.
$$
    \item the logarithm of a positive-definite operator $A$ (whose all eigenvalues are positive):
$$
\log (A) \equiv \sum_{a} \log (a)|a\rangle\langle a|.
$$
    \item the exponential of a normal operator $A$ . 
$$
\exp (A) \equiv \sum_{a} \exp (a)|a\rangle\langle a|.
$$
\end{itemize}
In general, we often define the matrix exponential for any square matrices $A$ (not necessarily to be normal) as
$$
\exp (A) =\sum_{k=0}^{\infty} \frac{1}{k!} A^k.
$$
Indeed, for the normal matrix, the two definition are equivalent. Let $A=\sum_{a} a|a\rangle\langle a|$ be a spectral decomposition for a normal operator $A$. Then,
\begin{align}
    A^2
    &=\left(  \sum_{a} a|a\rangle\langle a| \right)\left(  \sum_{a^{\prime}} a^{\prime}|a^{\prime}\rangle\langle a^{\prime}| \right) \\
    &= \sum_{a}  \sum_{a^{\prime}}  a a^{\prime}|a\rangle\langle a|a^{\prime}\rangle\langle a^{\prime}| \\
    &=\sum_{a} a^2|a\rangle\langle a|.
\end{align}
Similarly, we have $A^k=\sum_{a} a^k|a\rangle\langle a|$. Then, we have
\begin{align}
\exp (A) 
&=\sum_{k=0}^{\infty} \frac{1}{k!} A^k \\
&=\sum_{k=0}^{\infty} \frac{1}{k!}  \left(\sum_{a} a^k|a\rangle\langle a|\right)\\
&=\sum_{a} \left( \sum_{k=0}^{\infty} \frac{1}{k!}  a^k \right)|a\rangle\langle a|\\
&=\sum_{a} \exp (a)|a\rangle\langle a|.
\end{align}
See \href{https://en.wikipedia.org/wiki/Matrix_exponential}{wiki} for more nice properties of matrix exponential function. If a matrix is diagonal:
$$
A=\left[\begin{array}{cccc}
a_1 & 0 & \cdots & 0 \\
0 & a_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_n
\end{array}\right],
$$
then its exponential can be obtained by exponentiating each entry on the main diagonal:
$$
e^A=\left[\begin{array}{cccc}
e^{a_1} & 0 & \cdots & 0 \\
0 & e^{a_2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & & a_n
\end{array}\right] .
$$
As an example,
$$
\exp (\theta Z)=\left[\begin{array}{cc}
e^{\theta} & 0 \\
0 & e^{-\theta}
\end{array}\right].
$$
In the other hand, we obtain the same result by definition $\exp (A) \equiv \sum_{a} \exp (a)|a\rangle\langle a|.$

\begin{exercise}
Exercise 2.34: Find the square root and logarithm of the matrix
$$
\left[\begin{array}{ll}
4 & 3 \\
3 & 4
\end{array}\right]
$$
\end{exercise}

\begin{exercise}
Exercise 2.35: (Exponential of the Pauli matrices) Let $\vec{v}$ be any real, three-dimensional unit vector and $\theta$ a real number. Prove that
$$
\exp (i \theta \vec{v} \cdot \vec{\sigma})=\cos (\theta) I+i \sin (\theta) \vec{v} \cdot \vec{\sigma}
$$

where $\vec{v} \cdot \vec{\sigma} \equiv \sum_{i=1}^{3} v_{i} \sigma_{i}$. This exercise is generalized in Problem 2.1 on page 117.
\end{exercise}

\subsubsection{Trace}

The trace of a squared matrix $A$ is defined to be the sum of its diagonal elements,
$$
\operatorname{tr}(A) \equiv \sum_{i} A_{i i}.
$$
The trace is easily seen to be \textit{cyclic}, $\operatorname{tr}(A B)=\operatorname{tr}(B A)$, and linear, $\operatorname{tr}(A+B)=\operatorname{tr}(A)+\operatorname{tr}(B), \operatorname{tr}(z A)=z \operatorname{tr}(A)$, where $A$ and $B$ are arbitrary matrices, and $z$ is a complex number. 

Furthermore, from the cyclic property it follows that the trace of a matrix is invariant under the \textit{unitary similarity transformation} 
$$A \rightarrow U A U^{\dagger},$$
as $\operatorname{tr}\left(U A U^{\dagger}\right)=$ $\operatorname{tr}\left(U^{\dagger} U A\right)=\operatorname{tr}(A)$. In light of this result, it makes sense to define the \textit{trace of an operator $A$} to be the trace of any matrix representation of $A$.

As an example of the trace, suppose $|\psi\rangle$ is a unit vector and $A$ is an arbitrary operator. To evaluate $\operatorname{tr}(A|\psi\rangle\langle\psi|)$, use the Gram-Schmidt procedure to extend $|\psi\rangle$ to an orthonormal basis $|i\rangle$ which includes $|\psi\rangle$ as the first element. Then we have
$$
\begin{aligned}
\operatorname{tr}(A|\psi\rangle\langle\psi|) 
& =\sum_{i}\langle i| \left(A| \psi\rangle\langle\psi | \right) | i\rangle \\
& =\sum_{i}\langle i| A| \psi\rangle\langle\psi |  i\rangle \\
& =\sum_{i}\left(\langle\psi |  i\rangle\langle i| \right) A| \psi\rangle \\
& =\langle\psi|A| \psi\rangle.
\end{aligned}
$$
This result, that 
$$
\operatorname{tr}(A|\psi\rangle\langle\psi|)=\langle\psi|A| \psi\rangle
$$
is extremely useful in evaluating the trace of an operator. Again, this result directly comes from the cyclic property of the trace.


\begin{exercise}
Exercise 2.36: Show that the Pauli matrices except for $I$ have trace zero.
\end{exercise}

\begin{exercise}
Exercise 2.37: (Cyclic property of the trace) If $A$ and $B$ are two linear operators show that $\operatorname{tr}(A B)=\operatorname{tr}(B A).$
\end{exercise}

\begin{exercise}
Exercise 2.38: (Linearity of the trace) If $A$ and $B$ are two linear operators, show that $\operatorname{tr}(A+B)=\operatorname{tr}(A)+\operatorname{tr}(B)$ and if $z$ is an arbitrary complex number show that $\operatorname{tr}(z A)=z \operatorname{tr}(A)$.
\end{exercise}

\begin{exercise}
Exercise 2.39: (The Hilbert-Schmidt inner product on operators) The set $L_{V}$ of linear operators on a Hilbert space $V$ is obviously a vector space over $\mathbb{C}$- the sum of two linear operators is a linear operator, $z A$ is a linear operator if $A$ is a linear operator and $z$ is a complex number, and there is a zero element 0 . 

An important additional result is that the vector space $L_{V}$ can be given a natural inner product structure, turning it into a Hilbert space.
\begin{enumerate}
    \item Show that the function $(\cdot, \cdot)$ on $L_{V} \times L_{V}$ defined by
$$
(A, B) \equiv \operatorname{tr}\left(A^{\dagger} B\right)
$$
is an inner product function. This inner product is known as the Hilbert-Schmidt or trace inner product.
\item If $V$ has $d$ dimensions show that $L_{V}$ has dimension $d^{2}$.
\item Find an orthonormal basis of Hermitian matrices for the Hilbert space $L_{V}$.
\end{enumerate}
\end{exercise}